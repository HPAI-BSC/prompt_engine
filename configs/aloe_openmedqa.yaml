vllm:
  model: HPAI-BSC/Llama3.1-Aloe-Beta-8B
  tokenizer_mode:
  trust_remote_code: true
  tensor_parallel_size:
  dtype:
  quantization:
  revision:
  tokenizer_revision:
  seed: 42
  gpu_memory_utilization:
  swap_space:
  enforce_eager:
  max_context_len_to_capture:
  disable_custom_all_reduce:
  max_model_len:

config:
  working_dir: /gpfs/projects/bsc70/hpai/storage/data/jordi/medprompt
  eval: True
  dataset: openmedqa
  final_answer: reflection
  ensembles: 3
  k: 3
  embedding: NeuML/pubmedbert-base-embeddings
  database: your_custom_database

sampling_params:
  n:
  best_of:
  presence_penalty:
  frequency_penalty:
  repetition_penalty:
  temperature: 0.6
  top_p: 0.9
  top_k:
  min_p:
  seed:
  use_beam_search:
  length_penalty:
  early_stopping:
  stop: ["<|im_end|>", "[/INST]", "</s>", "<|eot_id|>", <|endoftext|>]
  stop_token_ids:
  include_stop_str_in_output:
  ignore_eos:
  max_tokens: 2000
  logprobs:
  prompt_logprobs:
  skip_special_tokens:
  spaces_between_special_tokens:
  logits_processors:

evaluators:
  - model: deepseek-ai/DeepSeek-R1
    trust_remote_code: True
    tensor_parallel_size:
    dtype:
    seed:
    gpu_memory_utilization:
    enforce_eager:
    max_model_len: